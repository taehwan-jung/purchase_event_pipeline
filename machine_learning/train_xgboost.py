import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import mean_absolute_error, r2_score # í‰ê°€ ì§€í‘œ ë³€ê²½
from pyspark.sql import SparkSession
from config.config import AWS_SECRET_ACCESS_KEY, AWS_ACCESS_KEY_ID
import os



# load from s3
def load_data_from_s3():
    spark = SparkSession.builder \
        .appName("LoadForML") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .getOrCreate()
    
    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
    hadoop_conf.set("fs.s3a.access.key", AWS_ACCESS_KEY_ID)
    hadoop_conf.set("fs.s3a.secret.key", AWS_SECRET_ACCESS_KEY)
    hadoop_conf.set("fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com")
    
    # s3ì—ì„œ ì½ê¸°
    df_spark = spark.read.parquet("s3a://purchase-pipeline/purchase_data_mart")

    # pandasë¡œ ë³€í™˜
    pdf = df_spark.toPandas()
    spark.stop()
    return pdf 

# main
def main():

    # ë°ì´í„°ë¡œë“œ
    data = load_data_from_s3()
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)} í–‰")
    
    target_column = 'avg_purchase_interval' 
    
    # 3. í”¼ì²˜ ì„ íƒ (ìƒˆë¡œ ë§Œë“  ì£¼ê¸° í”¼ì²˜ë“¤ ì¶”ê°€) â­ï¸
    features = [
        'recency', 'frequency', 'avg_order_value', 
        'avg_shopping_hour', 'weekend_purchase_ratio', 'distinct_item_count',
        'std_purchase_interval', 'last_purchase_interval'
    ]
    
    X = data[features]
    y = data[target_column] # ì¼ìˆ˜(Days) ì˜ˆì¸¡

    # 4. ë°ì´í„°ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)
    
    # 5. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ (Regressorë¡œ ë³€ê²½) â­ï¸
    model = xgb.XGBRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        objective='reg:squarederror', # íšŒê·€ìš© ëª©ì  í•¨ìˆ˜
        random_state=42
    )
    
    print("ğŸš€ êµ¬ë§¤ ì£¼ê¸° ì˜ˆì¸¡ ëª¨ë¸(Regression) í•™ìŠµ ì‹œì‘...")
    model.fit(X_train, y_train)

    # 6. ì˜ˆì¸¡ ë° í‰ê°€ (í‰ê°€ ì§€í‘œ ë³€ê²½) â­ï¸
    y_pred = model.predict(X_test)
    print("\n [ëª¨ë¸ í‰ê°€ ê²°ê³¼]")
    # ë¶„ë¥˜ ë¦¬í¬íŠ¸ ëŒ€ì‹  ì˜¤ì°¨(Error)ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    mae = mean_absolute_error(y_test, y_pred)
    print(f"Mean Absolute Error: {mae:.2f} ì¼") # í‰ê· ì ìœ¼ë¡œ ë©°ì¹ ì´ë‚˜ í‹€ë¦¬ëŠ”ì§€
    print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")

    # 7. í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸ (ì–´ë–¤ ë°ì´í„°ê°€ ì£¼ê¸°ë¥¼ ë§ì¶”ëŠ”ë° ì¤‘ìš”í–ˆë‚˜?)
    importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)
    print("\n [í”¼ì²˜ ì¤‘ìš”ë„]")
    print(importances)

    # 8. í•™ìŠµ ëª¨ë¸ ì €ì¥
    model_dir = "/opt/spark-apps/models"
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    save_path = os.path.join(model_dir, "customer_model.json")
    model.save_model(save_path)
    print(f" \n ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")

if __name__ == "__main__":
    main()

