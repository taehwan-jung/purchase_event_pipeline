from airflow import DAG
import pandas as pd
from pyspark.sql import SparkSession
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta
import pandas as pd
import xgboost as xgb
from sqlalchemy import create_engine
import os
import sys
import pendulum

# ê²½ë¡œ ì„¤ì •
sys.path.append('/opt')
from config.config import AWS_SECRET_ACCESS_KEY, AWS_ACCESS_KEY_ID

# í•œêµ­ ì‹œê°„ ì„¤ì •
local_tz = pendulum.timezone("Asia/Seoul")

def train_purchase_interval_model():
    # Spark ì„¸ì…˜ ìƒì„±
    spark = SparkSession.builder \
        .appName("Airflow_ML_Training") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .getOrCreate()

    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
    hadoop_conf.set("fs.s3a.access.key", AWS_ACCESS_KEY_ID)
    hadoop_conf.set("fs.s3a.secret.key", AWS_SECRET_ACCESS_KEY)
    hadoop_conf.set("fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com")

    try:
        # S3 ë°ì´í„° ë§ˆíŠ¸ ë¡œë“œ
        df_spark = spark.read.parquet("s3a://purchase-pipeline/purchase_data_mart")
        data = df_spark.toPandas()
        spark.stop()

        # êµ¬ë§¤ ì£¼ê¸°ê°€ 0ë³´ë‹¤ í° ë°ì´í„°ë§Œ ë‚¨ê¹€ (0ì¸ ë°ì´í„°ëŠ” ëª¨ë¸í•™ìŠµì„ ë°©í•´í•¨)
        data = data[data['avg_purchase_interval'] > 0]

        print(f"âœ… S3 ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)} í–‰")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    if data.empty or len(data) < 10:
        print("âš  ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    # í”¼ì²˜ ë° íƒ€ê²Ÿ ì„¤ì • 
    target = 'avg_purchase_interval'
    features = [
        'recency', 'frequency', 'avg_order_value', 
        'avg_shopping_hour', 'weekend_purchase_ratio', 'distinct_item_count',
        'std_purchase_interval', 'last_purchase_interval' 
    ]
    
    X = data[features]
    y = data[target]

    # ëª¨ë¸ í•™ìŠµ (Regressor)    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Regressor
    model = xgb.XGBRegressor(
        n_estimators=100, 
        learning_rate=0.1, 
        max_depth=5, 
        objective='reg:squarederror', 
        random_state=42
    )
    
    print("ğŸš€ íšŒê·€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    model.fit(X_train, y_train)

    # í‰ê°€ ë¡œê·¸ ì¶œë ¥
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    print(f"âœ… í•™ìŠµ ì™„ë£Œ! í‰ê·  ì˜¤ì°¨(MAE): {mae:.2f}ì¼")

    # ëª¨ë¸ ì €ì¥ (FastAPIì™€ ê³µìœ ë˜ëŠ” ê²½ë¡œ)
    model_dir = "/opt/airflow/models"
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    save_path = os.path.join(model_dir, "customer_model.json")
    model.save_model(save_path)
    print(f"âœ… ëª¨ë¸ ìë™ ì €ì¥ ì„±ê³µ: {save_path}")

# DAG ì„¤ì •
default_args = {
    'owner': 'admin',
    'start_date': datetime(2023, 12, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'customer_vp_full_pipeline_v1',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
) as dag:

    preprocess_data = BashOperator(
        task_id='spark_preprocessing',
        bash_command='export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 && export PATH=$JAVA_HOME/bin:$PATH && python3 /opt/spark/spark_process.py',
        env={'MSYS_NO_PATHCONV': '1'}
    )

    train_model = PythonOperator(
        task_id='xgboost_training',
        python_callable=train_purchase_interval_model
    )

    preprocess_data >> train_model