import sys
import os


# ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ íƒìƒ‰ ì¶”ê°€
sys.path.append('/home/airflow/.local/lib/python3.8/site-packages')
sys.path.append('/opt')

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.functions import col, when, count, abs
from pyspark.sql.functions import max, countDistinct, sum, datediff, lit
from pyspark.sql.functions import hour, dayofweek, avg
from config.config import POSTGRES_URL, POSTGRES_PASSWORD, POSTGRES_TABLE, POSTGRES_USER, POSTGRES_JDBC_DRIVER, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY

# 1. Sparksession ìƒì„±
def create_spark_session():
    spark = (
        SparkSession.builder
            .appName("Spark_process")
            # .master("spark://spark-master:7077")
            .master("local[*]")
            .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,"
                    "org.postgresql:postgresql:42.7.3,"
                    "org.apache.hadoop:hadoop-aws:3.3.4,"
                    "com.amazonaws:aws-java-sdk-bundle:1.12.262"
            )
            .config("spark.sql.shuffle.partitions", 3)
            .getOrCreate()
    )

    # AWS ì¸ì¦ ì„¤ì •
    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
    hadoop_conf.set("fs.s3a.access.key", AWS_ACCESS_KEY_ID)
    hadoop_conf.set("fs.s3a.secret.key", AWS_SECRET_ACCESS_KEY)
    hadoop_conf.set("fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com") # ì„œìš¸
    hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")   

    spark.sparkContext.setLogLevel("WARN")
    return spark

# 2. ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ postgres ì—ì„œ ì½ê¸°
def load_raw_transaction(spark):
    try:
        df_raw = spark.read \
            .format("jdbc") \
            .option("url", POSTGRES_URL) \
            .option("dbtable", POSTGRES_TABLE) \
            .option("user", POSTGRES_USER) \
            .option("password", POSTGRES_PASSWORD) \
            .option("driver", POSTGRES_JDBC_DRIVER)\
            .load()
        
        print(f"âœ… PostgreSQL í…Œì´ë¸” '{POSTGRES_TABLE}'ì—ì„œ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        df_raw.printSchema()
        return df_raw
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# 3. í´ë Œì§•(ì·¨ì†Œ, ì´ìƒê°’, íƒ€ì…, íŒŒìƒì»¬ëŸ¼, ì¤‘ë³µì œê±°)

def cleanse_data(df):
    if df is None:
        return None
    
    print("ğŸš€ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ì¤‘ë³µ ì œê±°
    df_cleaned = df.dropDuplicates()

    # ì·¨ì†Œ ì£¼ë¬¸ ì²˜ë¦¬
    # InvoiceNo ì´ 'c'ì¸ê²ƒì€ ë°˜í’ˆ/ì·¨ì†Œ ë°ì´í„°
    df_cleaned = df_cleaned.filter(~col('invoice_no').startswith("C"))

    # ì´ìƒì¹˜ ì²˜ë¦¬ (ìˆ˜ëŸ‰ì´ 0ì´í•˜ì¸ê²ƒì€ ì´ìƒì¹˜ì„)
    df_cleaned = df_cleaned.filter((col("quantity")> 0) & (col("unit_price") > 0)) 

    # CustomerID ê²°ì¸¡ì¹˜ ì²˜ë¦¬(customerIDê°€ ì—†ëŠ” ê°’ì€ ì‚­ì œí•¨)
    df_cleaned = df_cleaned.dropna(subset=["customer_id"])

    # íŒŒìƒì»¬ëŸ¼ ìƒì„± (ì´ íŒë§¤ì•¡)
    df_cleaned = df_cleaned.withColumn("total_amount", col("quantity")* col("unit_price"))

    print(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ {df_cleaned.count()} ê°œì˜ í–‰ì´ ìœ ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return df_cleaned

# 4.RFM í”¼ì²˜ ìƒì„± (recency(ìµœê·¼ì„±), monetary(ê¸ˆì•¡), freqency(ë¹ˆë„))
def create_rfm_features(df_cleaned):
    if df_cleaned is None:
        return None
    print("ğŸ“ˆ RFM í”¼ì²˜ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # max_dateë¥¼ ì„¤ì •í•˜ëŠ” ì´ìœ ëŠ” ë°ì´í„°ê°€ ê³¼ê±°ì˜ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì—, ë§ˆì§€ë§‰ ë‚ ì§œê°€ ê°€ì¥ ìµœê·¼ì˜ ë°ì´í„°ë¼ëŠ” ì ì„ ì§€ì •í•´ì¤˜ì•¼í•¨ / ìŠ¤íŒŒí¬ ì‚¬ìš©í•˜ê¸°ì— collect() ì‚¬ìš©í•˜ê³ , litëŠ” ìƒìˆ˜ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
    max_date = df_cleaned.select(max("invoice_date")).collect()[0][0]
    reference_date = lit(max_date)

    # recency = ê¸°ì¤€ì¼ - ë§ˆì§€ë§‰ êµ¬ë§¤ì¼, frequency = ê³ ìœ í•œ ì£¼ë¬¸ ìˆ˜, monetary = ì´ êµ¬ë§¤ í•©ê³„ (ê³ ê°ë³„ ê´€ë ¨ í”¼ì²˜)
    rfm_df = df_cleaned.groupBy("customer_id").agg(datediff(reference_date, max("invoice_date")).alias("recency"),
                                                    countDistinct("invoice_no").alias("frequency"),
                                                      sum("total_amount").alias("monetary"))

    # ê³ ê°ë‹¹ í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡
    rfm_df = rfm_df.withColumn("avg_order_value", col("monetary") / col("frequency"))
    print(f"RFM í”¼ì²˜ ìƒì„± ì™„ë£Œ: {rfm_df.count()} ëª…ì˜ ê³ ê° ë°ì´í„°")
    return rfm_df

def create_purchase_interval_features(df_cleaned):
    # 1. ê³ ê°ë³„ë¡œ ì‹œê°„ì„ í•œ ì¤„ë¡œ ì„¸ìš°ëŠ” 'ê¸°ì¤€(Window)' ë§Œë“¤ê¸°
    # partitionBy: ê³ ê°ë³„ë¡œ ê·¸ë£¹ì„ ë¬¶ìŒ
    # orderBy: ê·¸ ì•ˆì—ì„œ ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
    window_spec = Window.partitionBy("customer_id").orderBy("invoice_date")
    
    # 2. ì£¼ë¬¸ì¼ìë§Œ ë½‘ì•„ì„œ ì¤‘ë³µ ì œê±° (ê°™ì€ ë‚  ì—¬ëŸ¬ ê°œ ì‚¬ë„ í•˜ë‚˜ì˜ 'ì£¼ë¬¸ ì‹œì 'ìœ¼ë¡œ ì·¨ê¸‰)
    df_intervals = df_cleaned.select("customer_id", "invoice_no", "invoice_date").distinct()
    
    # 3. [í•µì‹¬] F.lagë¥¼ ì‚¬ìš©í•´ 'ì§ì „ êµ¬ë§¤ì¼' ì»¬ëŸ¼ ìƒì„±
    # í˜„ì¬ í–‰ì˜ invoice_date ë°”ë¡œ ìœ„ì— ìˆëŠ” ê°’ì„ ê°€ì ¸ì™€ì„œ 'prev_invoice_date'ì— ë„£ìŒ
    df_intervals = df_intervals.withColumn(
        "prev_invoice_date", 
        F.lag("invoice_date").over(window_spec)
    )
    
    # 4. ë‘ ë‚ ì§œì˜ ì°¨ì´(Interval) ê³„ì‚°
    # datediff(ë‚˜ì¤‘ ë‚ ì§œ, ì´ì „ ë‚ ì§œ) -> ê²°ê³¼ëŠ” ì •ìˆ˜(ì¼ìˆ˜)
    df_intervals = df_intervals.withColumn(
        "days_since_last_purchase", 
        F.datediff(F.col("invoice_date"), F.col("prev_invoice_date"))
    )
    
    # 5. ê³ ê°ë³„ë¡œ êµ¬ë§¤ ê°„ê²©ë“¤ì˜ í†µê³„ëŸ‰ ê³„ì‚°
    # í•œ ê³ ê°ì´ 10ë²ˆ ìƒ€ë‹¤ë©´ ê°„ê²©ì€ 9ê°œê°€ ìƒê¹€. ì´ 9ê°œì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•¨
    interval_stats = df_intervals.groupBy("customer_id").agg(
        F.avg("days_since_last_purchase").alias("avg_purchase_interval"),    # í‰ê·  ì£¼ê¸°
        F.stddev("days_since_last_purchase").alias("std_purchase_interval"), # ì£¼ê¸° ì¼ê´€ì„±
        F.last("days_since_last_purchase").alias("last_purchase_interval")   # ê°€ì¥ ìµœê·¼ ì£¼ê¸°
    )
    
    return interval_stats

# XGboostìš© í”¼ì²˜ ìƒì„±
def create_final_features(df_cleaned, rfm_df):
    print("ğŸ§ª XGboostìš© ìµœì¢… í”¼ì²˜ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤....")

    # ì‹œê°„ ê´€ë ¨ í”¼ì³ ìƒì„±, 1(ì¼) ~ 7(í† ). ë³´í†µ 1,7ì¼ ì£¼ë§
    df_with_time = df_cleaned.withColumn("order_hour", hour(col("invoice_date"))) \
                             .withColumn("is_weekend", when(dayofweek(col("invoice_date")).isin(1,7), 1).otherwise(0))

    # ê³ ê°ë³„ ì‹œê°„ íŒ¨í„´ ì§‘ê³„
    # í‰ê·  êµ¬ë§¤ ì‹œê°„ëŒ€
    # ì£¼ë§ êµ¬ë§¤ ë¹„ìœ¨
    # êµ¬ë§¤í•œ ìœ ë‹ˆí¬í•œ ìƒí’ˆ ìˆ˜
    customer_time_features = df_with_time.groupBy("customer_id").agg(avg("order_hour").alias("avg_shopping_hour"), \
                                                                avg("is_weekend").alias("weekend_purchase_ratio"), \
                                                                countDistinct("stock_code").alias("distinct_item_count"))

    # RFM ë°ì´í„°ì™€ ì‹œê°„ íŒ¨í„´ ë°ì´í„° ê²°í•©
    final_mart = rfm_df.join(customer_time_features, on="customer_id", how="inner")

    # êµ­ê°€ ì •ë³´ ì¶”ê°€
    # customer_country = df_cleaned.select("customer_id" , "country").dropDuplicates(["customer_id"])
    # final_mart = final_mart.join(customer_country, on="customer_id", how="inner")

    print(f"âœ… ìµœì¢… í”¼ì²˜ ë§ˆíŠ¸ ìƒì„± ì™„ë£Œ!")
    return final_mart

# ê²°ê³¼ ì €ì¥ s3 save_to s3
def save_to_s3(df, bucket_name, folder_path):
    full_path = f"s3a://{bucket_name}/{folder_path}"
    print(f"ğŸ“¦ s3 ì €ì¥ ì¤‘... ê²½ë¡œ{full_path}")

    try:
        df.write.mode("overwrite").parquet(full_path)
        print("âœ… S3 ì €ì¥ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ S3 ì €ì¥ ì‹¤íŒ¨: {e}")

#  ê²°ê³¼ ì €ì¥ save_to_postgre 
def save_to_postgres(final_mart):
    """
    ì „ì²˜ë¦¬ëœ ë§ˆíŠ¸ ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    POSTGRES_MART_TABLE = "purchase_data_mart"  # ì›ë³¸ê³¼ ë‹¤ë¥¸ ì´ë¦„ í•„ìˆ˜!
    
    print(f"ğŸš€ [Mart Save] ë°ì´í„° ë§ˆíŠ¸ ì €ì¥ ì‹œì‘ (Rows: {final_mart.count()})")
    
    (
        final_mart.write
        .format("jdbc")
        .option("url", POSTGRES_URL)
        .option("dbtable", POSTGRES_MART_TABLE)  
        .option("user", POSTGRES_USER)
        .option("password", POSTGRES_PASSWORD)
        .option("driver", POSTGRES_JDBC_DRIVER)
        .mode("overwrite")
        .save()
    )
    print(f"âœ… [Mart Save] {POSTGRES_MART_TABLE} í…Œì´ë¸” ì €ì¥ ì™„ë£Œ!")

# main
def main():
    # ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±
    spark = create_spark_session()

    # ë°ì´í„° ë¡œë“œ
    df_raw = load_raw_transaction(spark)

    if df_raw is not None:
        df_cleaned = cleanse_data(df_raw) # ë°ì´í„° ì „ì²˜ë¦¬
                    
        # customer_id  ê°€ ì—†ëŠ” ê²½ìš° ì‚­ì œ
        df_cleaned = df_cleaned.filter(col("customer_id").isNotNull())

        if df_cleaned is not None:
            # recency, frequency, monetary ìƒì„± 
            rfm_df = create_rfm_features(df_cleaned)
            
            # êµ¬ë§¤ ê°„ê²© í”¼ì²˜ ìƒì„±
            interval_df = create_purchase_interval_features(df_cleaned)
            
            # customer_id ê¸°ì¤€ inner join
            combined_rfm = rfm_df.join(interval_df, on="customer_id", how="inner")
            
            # xgboostìš© ì‹œê°„íŒ¨í„´ ìƒì„± 
            final_mart = create_final_features(df_cleaned, combined_rfm)

            # ì¶”í›„ì— xgboost regression ì‚¬ìš©ì‹œ í•œ ë²ˆë§Œ êµ¬ë§¤í•œ ê³ ê°ì˜ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•¨ (ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°)
            final_mart = final_mart.na.fill({
            "avg_purchase_interval": 0, 
            "std_purchase_interval": 0,
            "last_purchase_interval": 0
            })

            # ìµœì¢… ë²„ì „ì—ì„œ joinì´í›„ naê°’ì´ ìˆëŠ” ê²½ìš° ì œê±° 
            final_mart = final_mart.dropna(how='any')

            # ê²°ì¸¡ì¹˜ë¥¼ ì œê±°í•´ì„œ float -> intë¡œ ë³€ê²½
            final_mart = final_mart.withColumn("customer_id", col("customer_id").cast("long"))

            final_mart.show(5)

            # Save to S3
            save_to_s3(final_mart, "purchase-pipeline" , "purchase_data_mart")      

            # Save tp postgres
            save_to_postgres(final_mart)      

    else:
        print("ğŸ›‘ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í•´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")

    spark.stop()

if __name__ == "__main__":
    main()