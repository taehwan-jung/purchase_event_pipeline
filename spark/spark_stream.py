from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
import time
from config.config import KAFKA_BOOTSTRAP_SERVERS, KAFKA_TOPIC, POSTGRES_URL, POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_TABLE
from utils.log_utils import setup_logger

# ë¡œê¹… ì„¤ì •
logger = setup_logger("spark_stream", "logs/spark_stream.log")

# Create spark
def create_spark_session():
    spark = (
        SparkSession.builder
            .appName("PurchaseEventConsumer")
            .master("spark://spark-master:7077")
            .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,"
                    "org.postgresql:postgresql:42.7.3"
            )
            .config("spark.sql.shuffle.partitions", 3)
            .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")
    return spark

# Schema ì„¤ì •
schema = StructType([
    StructField("invoice_no", StringType(), True),
    StructField("stock_code", StringType(), True),
    StructField("description", StringType(), True),
    StructField("quantity", IntegerType(), True),
    StructField("invoice_date", StringType(), True), # ë‚˜ì¤‘ì— timestampë¡œ 
    StructField("unit_price", DoubleType(), True),
    StructField("customer_id", StringType(), True),
    StructField("country", StringType(), True)

])


# Read from kafka
def read_from_kafka(spark):
    kafka_df = (
        spark.readStream
            .format("kafka")
            .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
            .option("subscribe", KAFKA_TOPIC)
            .option("startingOffsets", "earliest")
            .option("maxOffsetsPerTrigger", 5000)
            .load()
    )

    raw_df = kafka_df.selectExpr("CAST(value as STRING) as json_str")
    return raw_df

# Parsed_df
def parse_json(raw_df):
    parsed_df = (
        raw_df
            .select(from_json(col("json_str"), schema).alias("data"))
            .select("data.*")
    )
    return parsed_df


# invoice_date timestampë¡œ ë³€í™˜(ìœˆë„ìš°/ì›Œí„°ë§ˆí¬)
def add_timestamp_column(parsed_df):
    df_with_ts = (
        parsed_df
        .withColumn(
            "invoice_ts",
            to_timestamp(col("invoice_date"), "yyyy-MM-dd H:mm")
        )
        # invoice_date: DB ì»¬ëŸ¼ê³¼ íƒ€ì… ë§ì¶”ê¸° ìœ„í•´ timestampë¡œ ë³€í™˜
        .withColumn(
            "invoice_date",
            to_timestamp(col("invoice_date"), "yyyy-MM-dd H:mm")
        )
    )
    return df_with_ts

# postgreSQLì— ë°°ì¹˜ ë‹¨ìœ„ ì €ì¥
def write_to_postgres(batch_df, batch_id):
    print("ğŸ”¥ [foreachBatch] batch_id =", batch_id)    
    # 1) ì´ ë°°ì¹˜ì— ì‹¤ì œë¡œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    count = batch_df.count()
    print("ğŸ”¥ [foreachBatch] row count =", count)
    (
        batch_df.write
        .format("jdbc")
        .option("url", POSTGRES_URL)
        .option("dbtable", POSTGRES_TABLE)
        .option("user", POSTGRES_USER)
        .option("password", POSTGRES_PASSWORD)
        .option("driver", "org.postgresql.Driver")
        .mode("append")
        .save()
    )
    print(f"Batch{batch_id} saved to postgreSQL")
    print("ğŸ”¥ BATCH SIZE =", batch_df.count())
    batch_df.printSchema()



# Console query
def start_console_query(df):
    query = (
        df.writeStream
            .outputMode("append")
            .format("console")
            .option("truncate", False)
            .option("numRows", 20)
            .option("checkpointLocation", "/opt/spark/work-dir/checkpoints/console")
            .start()
    )
    return query 

# PoistgreSQL query
def start_postgres_query(df):
    return(
        df.writeStream
            .outputMode("append")
            .foreachBatch(write_to_postgres)
            .option("checkpointLocation", "/opt/spark/work-dir/checkpoints/postgres")
            .start()
    )


# def main
def main():
    # ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±
    spark = create_spark_session()

    # kafkaë¡œ json ìŠ¤íŠ¸ë§ ì½ê¸°
    raw_df = read_from_kafka(spark)

    # ìŠ¤í‚¤ë§ˆ ì ìš© json íŒŒì‹±
    parsed_df = parse_json(raw_df)

    # timestamp ì»¬ëŸ¼ ì¶”ê°€
    df_with_ts = add_timestamp_column(parsed_df)

    # console + postgresql ì‹¤í–‰
    console_query = start_console_query(df_with_ts)
    postgres_query = start_postgres_query(df_with_ts)

    # query.awaitTermination()
    time.sleep(60)  # 1ë¶„
    console_query.stop()
    postgres_query.stop()
    spark.stop()

    print("===== Final Postgres Query Progress =====")
    print(postgres_query.lastProgress)

if __name__ == "__main__":
    main()

